import google.generativeai as genai
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
import joblib
import os
import json
import re
from datetime import datetime

class NewsProcessorCompleto:
    def __init__(self):
        self.model_file = "relevance_model.pkl"
        self.vectorizer_file = "vectorizer.pkl" 
        self.feedback_file = "feedback.csv"
        self.data_file = "database/news_database.json"
        
        # Keywords ajustadas para portugu√™s
        self.KEYWORDS = [
            # Seguros mar√≠timos e riscos
            "seguro mar√≠timo", "sinistro naval", "avaria", "indeniza√ß√£o mar√≠tima",
            "risco mar√≠timo", "seguradora mar√≠tima", "ap√≥lice mar√≠tima",
        
            # Portos brasileiros espec√≠ficos
            "porto de Itaqui", "porto do Pec√©m", "porto de Suape", "porto de Santos",
            "porto de Paranagu√°", "porto de Rio Grande", "porto de S√£o Lu√≠s",
        
            # Regulamenta√ß√£o e √≥rg√£os
            "ANTAQ", "Marinha do Brasil", "DPC", "Capitania dos Portos",
            "regulamenta√ß√£o portu√°ria", "normativa portu√°ria", "legisla√ß√£o mar√≠tima",
        
            # Opera√ß√µes portu√°rias
            "cabotagem", "navega√ß√£o interior", "hidrovia", "transporte aquavi√°rio",
            "terminal portu√°rio", "movimenta√ß√£o portu√°ria", "opera√ß√µes portu√°rias",
        
            # Regi√µes de atua√ß√£o
            "Maranh√£o", "Cear√°", "Amap√°", "Par√°", "Nordeste", "Norte",
            "S√£o Lu√≠s", "Fortaleza", "Macap√°", "Bel√©m",
        
            # Acidentes e incidentes
            "acidente naval", "naufr√°gio", "colis√£o naval", "incidente portu√°rio",
            "acidente portu√°rio", "avaria em navio"
        ]
        
        self.NEGATIVE_KEYWORDS = [
            "global", "internacional", "EUA", "China", "Europa", "√Åsia",
            "cruzeiro", "turismo", "pesca esportiva", "iate", "veleiro",
            "hist√≥rico", "antigo", "cultural", "festival", "entretenimento"
        ]
        
        self.setup_gemini()
        self.setup_ml_system()
    
    def setup_gemini(self):
        """Configura Gemini"""
        api_key = os.getenv("GEMINI_API_KEY")
        if api_key and api_key != "sua_chave_gemini_aqui":
            try:
                genai.configure(api_key=api_key)
                self.model_flash = genai.GenerativeModel('gemini-1.5-flash')
                self.gemini_enabled = True
                print("‚úÖ Gemini Configurado")
            except Exception as e:
                print(f"‚ùå Erro configurando Gemini: {e}")
                self.gemini_enabled = False
        else:
            self.gemini_enabled = False
            print("‚ö†Ô∏è Gemini n√£o configurado - usando apenas filtros b√°sicos")
    
    def setup_ml_system(self):
        """Sistema de ML"""
        self.ml_model, self.ml_vectorizer = self.load_ml_model()
        if self.ml_model is None:
            print("üîß Modelo ML ser√° treinado quando houver feedback suficiente")
    
    def executar_coleta_completa(self):
        """Executa processamento COMPLETO"""
        print("üöÄ INICIANDO PROCESSAMENTO COMPLETO BRAZMAR")
        
        try:
            from scraper import fetch_rss, fetch_scrape
            artigos_rss = fetch_rss()
            artigos_scrape = fetch_scrape()
            artigos = artigos_rss + artigos_scrape
            print(f"üì∞ {len(artigos)} not√≠cias coletadas inicialmente")
        except Exception as e:
            print(f"‚ùå Erro na coleta: {e}")
            return []
        
        # Pr√©-processamento
        artigos_processados = self.preprocessar_artigos(artigos)
        
        # Filtragem
        artigos_relevantes = self.filtrar_artigos(artigos_processados)
        print(f"‚úÖ {len(artigos_relevantes)} not√≠cias relevantes encontradas")
        
        # Salvar no database
        self.salvar_no_database(artigos_relevantes)
        
        return artigos_relevantes
    
    def preprocessar_artigos(self, artigos):
        """Pr√©-processamento dos artigos"""
        artigos_processados = []
        
        for artigo in artigos:
            processed_artigo = artigo.copy()
            
            # Garante campos obrigat√≥rios
            processed_artigo['title'] = processed_artigo.get('title', 'Sem T√≠tulo').strip()
            processed_artigo['summary'] = processed_artigo.get('summary', '').strip()
            processed_artigo['source'] = processed_artigo.get('source', 'Unknown').strip()
            
            # Limpeza b√°sica
            processed_artigo['summary'] = re.sub(r'http\S+', '', processed_artigo['summary'])
            processed_artigo['summary'] = ' '.join(processed_artigo['summary'].split())
            
            # Se summary muito curto, usa t√≠tulo
            if len(processed_artigo['summary']) < 20:
                processed_artigo['summary'] = processed_artigo['title']
            
            artigos_processados.append(processed_artigo)
        
        return artigos_processados
    
    def filtrar_artigos(self, artigos):
        """Filtragem H√çBRIDA (ML + Gemini)"""
        artigos_relevantes = []
        
        for i, artigo in enumerate(artigos):
            print(f"üîç Analisando {i+1}/{len(artigos)}: {artigo['title'][:50]}...")
            
            # Filtro autom√°tico
            relevante_auto = self.filtro_automatico(artigo)
            if not relevante_auto:
                print("   ‚ùå Rejeitado pelo filtro autom√°tico")
                continue
            
            # Filtro ML (se dispon√≠vel)
            if self.ml_model is not None:
                relevante_ml = self.filtrar_por_ml(artigo)
                if not relevante_ml:
                    print("   ‚ùå Rejeitado pelo ML")
                    continue
            
            # Gemini (se dispon√≠vel)
            if self.gemini_enabled:
                relevante_gemini = self.filtrar_por_gemini(artigo)
                if not relevante_gemini:
                    print("   ‚ùå Rejeitado pelo Gemini")
                    continue
                else:
                    print("   ‚úÖ Aprovado pelo Gemini")
            else:
                print("   ‚úÖ Aprovado pelos filtros b√°sicos")
            
            # Adiciona metadados
            artigo['processed_at'] = datetime.now().isoformat()
            artigo['collection_date'] = datetime.now().strftime("%Y-%m-%d")
            artigo['ai_analyzed'] = self.gemini_enabled
            
            # Define urg√™ncia padr√£o se n√£o definida pelo Gemini
            if 'urgencia' not in artigo:
                artigo['urgencia'] = 'MEDIA'
            if 'confianca' not in artigo:
                artigo['confianca'] = 70
                
            artigos_relevantes.append(artigo)
        
        return artigos_relevantes
    
    def filtro_automatico(self, artigo):
        """Filtro autom√°tico por keywords"""
        combined_text = (artigo['title'] + " " + artigo['summary']).lower()
        score = sum(1 for kw in self.KEYWORDS if kw.lower() in combined_text)
        
        # Penaliza por negative keywords
        negative_score = sum(1 for nkw in self.NEGATIVE_KEYWORDS if nkw.lower() in combined_text)
        score -= negative_score * 2
        
        print(f"   üîß Score autom√°tico: {score}")
        return score >= 2  # Reduzido o threshold para capturar mais not√≠cias
    
    def filtrar_por_ml(self, artigo):
        """Filtro por Machine Learning"""
        if self.ml_model is None or self.ml_vectorizer is None:
            return True
        
        try:
            combined_text = artigo['title'] + " " + artigo['summary']
            features = self.ml_vectorizer.transform([combined_text])
            prediction = self.ml_model.predict(features)[0]
            probability = self.ml_model.predict_proba(features)[0][1]
            
            print(f"   ü§ñ ML - Predi√ß√£o: {prediction}, Probabilidade: {probability:.2f}")
            return prediction == 1 and probability > 0.5
            
        except Exception as e:
            print(f"   ‚ùå Erro ML: {e}")
            return True
    
    def filtrar_por_gemini(self, artigo):
        """Filtro por Gemini AI"""
        try:
            prompt = f"""
            ANALISAR para BRAZMAR MARINE SERVICES (seguros mar√≠timos, consultoria portu√°ria):

            T√çTULO: {artigo['title']}
            RESUMO: {artigo['summary']}

            CRIT√âRIOS ESTRITOS - A not√≠cia deve ser sobre:
            ‚úÖ Seguros mar√≠timos, sinistros navais, avarias
            ‚úÖ Portos brasileiros (Itaqui, Pec√©m, Suape, Santos, etc.)
            ‚úÖ ANTAQ, Marinha do Brasil, regulamenta√ß√£o portu√°ria
            ‚úÖ Acidentes/incidentes em portos ou navios
            ‚úÖ Opera√ß√µes de cabotagem, navega√ß√£o interior
            ‚úÖ Regi√£o Norte/Nordeste do Brasil

            ‚ùå REJEITAR se for sobre:
            ‚ùå Turismo, cruzeiros, pesca esportiva
            ‚ùå Not√≠cias internacionais
            ‚ùå Entretenimento, cultura, eventos
            ‚ùå Assuntos gerais sem liga√ß√£o direta com opera√ß√µes mar√≠timas

            Responda APENAS com JSON:
            {{
                "relevante": true/false,
                "confianca": 0-100,
                "motivo": "explica√ß√£o espec√≠fica",
                "urgencia": "BAIXA/MEDIA/ALTA"
            }}
            """
            
            response = self.model_flash.generate_content(prompt)
            analysis = self.parse_resposta_gemini(response.text)
            
            if analysis.get('relevante', False):
                artigo['gemini_analysis'] = analysis
                artigo['confianca'] = analysis.get('confianca', 0)
                artigo['urgencia'] = analysis.get('urgencia', 'BAIXA')
                print(f"   ‚úÖ Gemini - Confian√ßa: {analysis.get('confianca', 0)}%")
                return True
            else:
                print(f"   ‚ùå Gemini - Motivo: {analysis.get('motivo', 'N/A')}")
                return False
                
        except Exception as e:
            print(f"   ‚ùå Erro Gemini: {e}")
            return True
    
    def parse_resposta_gemini(self, response_text):
        """Parse da resposta do Gemini"""
        try:
            cleaned = re.sub(r'```json|```', '', response_text).strip()
            json_match = re.search(r'\{[^}]*\}', cleaned, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except Exception as e:
            print(f"   ‚ùå Erro parse Gemini: {e}")
        
        return {"relevante": False, "confianca": 0, "motivo": "Erro na an√°lise", "urgencia": "BAIXA"}
    
    def load_ml_model(self):
        """Carrega modelo ML"""
        if os.path.exists(self.model_file) and os.path.exists(self.vectorizer_file):
            try:
                model = joblib.load(self.model_file)
                vectorizer = joblib.load(self.vectorizer_file)
                print("‚úÖ Modelo ML carregado do cache")
                return model, vectorizer
            except Exception as e:
                print(f"‚ùå Erro carregando ML: {e}")
        return None, None
    
    def train_ml_model(self):
        """Treina modelo ML com feedback"""
        if not os.path.exists(self.feedback_file) or os.path.getsize(self.feedback_file) < 50:
            print("üìä CSV de feedback vazio ou insuficiente")
            return None, None
        
        try:
            df = pd.read_csv(self.feedback_file)
            
            if 'relevant' not in df.columns or len(df) < 5:
                print("‚ö†Ô∏è Dados insuficientes para treinar ML")
                return None, None
            
            df['text'] = df['title'].fillna('') + " " + df['summary'].fillna('')
            texts = df['text'].tolist()
            labels = df['relevant'].astype(bool).tolist()
            
            pipeline = Pipeline([
                ('tfidf', TfidfVectorizer(max_features=500, stop_words='portuguese')),
                ('clf', LogisticRegression())
            ])
            
            pipeline.fit(texts, labels)
            joblib.dump(pipeline.named_steps['clf'], self.model_file)
            joblib.dump(pipeline.named_steps['tfidf'], self.vectorizer_file)
            print("‚úÖ Modelo ML treinado com feedback")
            return pipeline.named_steps['clf'], pipeline.named_steps['tfidf']
            
        except Exception as e:
            print(f"‚ùå Erro treinamento ML: {e}")
            return None, None
    
    def salvar_no_database(self, artigos):
        """Salva artigos no database JSON"""
        os.makedirs("database", exist_ok=True)
        
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except FileNotFoundError:
            data = {"articles": [], "stats": {}}
        
        # Adiciona novos artigos (evita duplicatas)
        titulos_existentes = {a['title'] for a in data['articles']}
        novos = 0
        
        for artigo in artigos:
            if artigo['title'] not in titulos_existentes:
                data['articles'].append(artigo)
                novos += 1
        
        # Mant√©m apenas √∫ltimos 200 artigos
        data['articles'] = data['articles'][-200:]
        
        # Atualiza estat√≠sticas
        hoje = datetime.now().strftime("%Y-%m-%d")
        artigos_hoje = [a for a in data['articles'] if a.get('collection_date') == hoje]
        
        data['stats'] = {
            "total_articles": len(data['articles']),
            "today_articles": len(artigos_hoje),
            "last_updated": datetime.now().isoformat()
        }
        
        data['last_updated'] = datetime.now().isoformat()
        
        with open(self.data_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Database atualizado: {novos} novos artigos (total: {len(data['articles'])})")