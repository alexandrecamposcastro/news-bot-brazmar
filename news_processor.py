import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
import joblib
import os
import json
import re
from datetime import datetime
import time

# Importar providers novos
from gemini_provider import gemini_provider
from circular_expert import circular_expert
from database_hybrid import db
from history_manager import history_manager

class NewsProcessorCompleto:
    def __init__(self):
        self.model_file = "relevance_model.pkl"
        self.data_file = "database/news_database.json"
        
        # KEYWORDS ESPEC√çFICAS NORTE/NORDESTE
        self.REGIAO_KEYWORDS = [
            'norte', 'nordeste', 'maranh√£o', 'cear√°', 'piau√≠', 'bahia', 
            'pernambuco', 'alagoas', 'sergipe', 'para√≠ba', 'rio grande do norte',
            'amap√°', 'par√°', 'amazonas', 'rond√¥nia', 'acre', 'roraima', 'tocantins',
            'itaqui', 'pecem', 'suape', 's√£o lu√≠s', 'fortaleza', 'bel√©m', 'macap√°',
            'manaus', 'recife', 'salvador', 'natal', 'jo√£o pessoa'
        ]
        
        self.setup_ml_system()

    def setup_ml_system(self):
        """Sistema ML opcional"""
        try:
            if os.path.exists(self.model_file):
                self.ml_model = joblib.load(self.model_file)
                print("‚úÖ Modelo ML carregado")
            else:
                self.ml_model = None
                print("üîß ML ser√° treinado quando houver dados")
        except:
            self.ml_model = None

    def executar_coleta_completa(self):
        """Processamento OTIMIZADO - menos chamadas Gemini"""
        print("üöÄ INICIANDO COLETA BRAZMAR - FOCO NORTE/NORDESTE")
        
        try:
            from scraper import fetch_rss, fetch_scrape
            artigos_rss = fetch_rss()
            artigos_scrape = fetch_scrape()
            todos_artigos = artigos_rss + artigos_scrape
            
            print(f"üì∞ {len(todos_artigos)} not√≠cias coletadas")
        except Exception as e:
            print(f"‚ùå Erro coleta: {e}")
            return []

        # PR√â-FILTRO RIGOROSO ANTES de chamar Gemini
        artigos_pre_filtrados = self.pre_filtro_rigoroso(todos_artigos)
        print(f"üîç Pr√©-filtro: {len(todos_artigos)} ‚Üí {len(artigos_pre_filtrados)}")

        # S√≥ chama Gemini para os que passaram no pr√©-filtro
        artigos_relevantes = self.filtrar_com_gemini(artigos_pre_filtrados)
        print(f"‚úÖ Gemini: {len(artigos_relevantes)} not√≠cias relevantes")

        # GERA CIRCULAR
        if artigos_relevantes:
            circular = circular_expert.generate_circular(artigos_relevantes)
            self.salvar_circular(circular)
            print("üì® CIRCULAR GERADA COM SUCESSO!")

        # Salva resultados
        self.salvar_no_database(artigos_relevantes)
        
        return artigos_relevantes

    def pre_filtro_rigoroso(self, artigos):
        """Pr√©-filtro MUITO rigoroso - evita chamadas desnecess√°rias ao Gemini"""
        artigos_filtrados = []
        
        for artigo in artigos:
            texto = (artigo['title'] + ' ' + artigo.get('summary', '')).lower()
            
            # ‚úÖ DEVE ter palavra da regi√£o
            tem_regiao = any(keyword in texto for keyword in self.REGIAO_KEYWORDS)
            
            # ‚ùå N√ÉO PODE ter palavras de outras regi√µes
            outras_regioes = ['santos', 'rio de janeiro', 's√£o paulo', 'paranagu√°', 'rio grande', 'sul', 'sudeste']
            nao_tem_outras = not any(regiao in texto for regiao in outras_regioes)
            
            if tem_regiao and nao_tem_outras:
                artigos_filtrados.append(artigo)
        
        return artigos_filtrados

    def filtrar_com_gemini(self, artigos):
        """Usa Gemini APENAS para os artigos pr√©-filtrados"""
        artigos_relevantes = []
        
        for i, artigo in enumerate(artigos):
            print(f"üîç Gemini analisando {i+1}/{len(artigos)}: {artigo['title'][:50]}...")
            
            analysis = gemini_provider.analyze_article(artigo['title'], artigo.get('summary', ''))
            
            if analysis.get('relevante', False):
                # Adiciona metadados da an√°lise
                artigo.update({
                    'processed_at': datetime.now().isoformat(),
                    'collection_date': datetime.now().strftime("%Y-%m-%d"),
                    'confianca': analysis.get('confianca', 0),
                    'urgencia': analysis.get('urgencia', 'MEDIA'),
                    'ia_analysis': analysis
                })
                artigos_relevantes.append(artigo)
                print(f"   ‚úÖ Aprovado ({analysis.get('confianca', 0)}% confian√ßa)")
            else:
                print(f"   ‚ùå Rejeitado: {analysis.get('motivo', 'N/A')}")
        
        return artigos_relevantes

    def salvar_circular(self, circular):
        """Salva circular em arquivo"""
        try:
            os.makedirs("circulars", exist_ok=True)
            data = datetime.now().strftime("%Y-%m-%d")
            filename = f"circulars/brazmar_circular_{data}.txt"
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(circular)
            
            print(f"üíæ Circular salva: {filename}")
        except Exception as e:
            print(f"‚ùå Erro salvando circular: {e}")

    def salvar_no_database(self, artigos):
        """Salva artigos no database"""
        os.makedirs("database", exist_ok=True)
        
        # Salva no JSON
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except FileNotFoundError:
            data = {"articles": [], "stats": {}}

        # Adiciona novos (evita duplicatas)
        titulos_existentes = {a['title'] for a in data['articles']}
        novos = 0
        
        for artigo in artigos:
            if artigo['title'] not in titulos_existentes:
                data['articles'].append(artigo)
                novos += 1
                # Adiciona ao hist√≥rico
                history_manager.add_to_history(artigo)

        # Mant√©m s√≥ √∫ltimos 200
        data['articles'] = data['articles'][-200:]
        
        # Atualiza stats
        hoje = datetime.now().strftime("%Y-%m-%d")
        artigos_hoje = [a for a in data['articles'] if a.get('collection_date') == hoje]
        
        data['stats'] = {
            "total_articles": len(data['articles']),
            "today_articles": len(artigos_hoje),
            "last_updated": datetime.now().isoformat()
        }

        with open(self.data_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        # Salva no banco tamb√©m
        for artigo in artigos:
            db.save_article(artigo)

        print(f"üíæ Database: {novos} novos artigos salvos")

# Inst√¢ncia global
news_processor = NewsProcessorCompleto()